{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération des codes et merge avec la BDD existante\n",
    "\n",
    "## Traitements\n",
    "\n",
    "\n",
    "1. Chargement de la bdd existante et de la nouvelle généré à l'étape 1 (au même format)\n",
    "2. Merge des données sur matricule, nom, prénom, date de naissance du bénéficiaire (INNER)\n",
    "3. On garde les nouvelles données pour le résultat du merge\n",
    "3. Ajout des nouvelles données qui n'étaient pas présentes\n",
    "4. Génération des codes manquants pour les nouvelles lignes de bénéficiaires\n",
    "5. Output to csv\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "\n",
    "Création des bénéficiaires manuellement à partir du 27 Juin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "existing_db_export_filepath = os.environ['EXISTING_DB']\n",
    "new_db_export_filepath = os.environ['DB_EXPORT']\n",
    "concatenated_db_filepath = os.environ['DB_CONCATENATED']\n",
    "\n",
    "MEMORY_OPTIMIZATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_type = {\n",
    "    'qpv': 'boolean',\n",
    "    'a_valider': 'boolean',\n",
    "    'zrr': 'boolean',\n",
    "}\n",
    "df_existing_db = pd.read_csv(existing_db_export_filepath, index_col=0, sep=',',  dtype=column_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: suppress +4 with actual DB and keep casting to datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "df_existing_db['date_naissance'] = pd.to_datetime(df_existing_db['date_naissance']) + timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_allocataire = pd.json_normalize(df_existing_db['allocataire'].apply(json.loads))\n",
    "df_json_allocataire = df_json_allocataire.add_prefix('allocataire-')\n",
    "df_existing_db.index = pd.RangeIndex(start=0, stop=len(df_existing_db), step=1)\n",
    "df_existing_db_unwrapped_alloc = pd.merge(df_existing_db, df_json_allocataire, left_index=True, right_index=True)\n",
    "df_existing_db_unwrapped_alloc = df_existing_db_unwrapped_alloc.drop(columns=['allocataire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_existing_db\n",
    "    del df_json_allocataire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data casting and formating\n",
    "df_existing_db_unwrapped_alloc['allocataire-matricule'] = df_existing_db_unwrapped_alloc['allocataire-matricule'].astype(str)\n",
    "df_existing_db_unwrapped_alloc['allocataire-nom'] = df_existing_db_unwrapped_alloc['allocataire-nom'].str.upper()\n",
    "df_existing_db_unwrapped_alloc['allocataire-prenom'] = df_existing_db_unwrapped_alloc['allocataire-prenom'].str.upper()\n",
    "\n",
    "# cleaning (telephone)\n",
    "df_existing_db_unwrapped_alloc['allocataire-telephone'] = df_existing_db_unwrapped_alloc['allocataire-telephone'].replace('0', '')\n",
    "mask_tel_not_null = df_existing_db_unwrapped_alloc['allocataire-telephone'].notna()\n",
    "mask_no_zero_phone_number = ~df_existing_db_unwrapped_alloc.loc[mask_tel_not_null, 'allocataire-telephone'].str.startswith('0')\n",
    "mask_9_char_phone = df_existing_db_unwrapped_alloc.loc[mask_tel_not_null, 'allocataire-telephone'].str.len() == 9\n",
    "df_existing_db_unwrapped_alloc.loc[mask_tel_not_null & mask_no_zero_phone_number & mask_9_char_phone, 'allocataire-telephone'] = '0' + df_existing_db_unwrapped_alloc['allocataire-telephone']\n",
    "\n",
    "# replace blank string with nan\n",
    "df_existing_db_unwrapped_alloc = df_existing_db_unwrapped_alloc.replace(r'', np.NaN)\n",
    "df_existing_db_unwrapped_alloc = df_existing_db_unwrapped_alloc.replace(r'0', np.NaN)\n",
    "df_existing_db_unwrapped_alloc = df_existing_db_unwrapped_alloc.replace(r'00000', np.NaN)\n",
    "\n",
    "# lower case on email\n",
    "df_existing_db_unwrapped_alloc['allocataire-courriel'] = df_existing_db_unwrapped_alloc['allocataire-courriel'].str.lower()\n",
    "\n",
    "# check all columns exists, create them otherwise (appens if we act on CNOUS only data)\n",
    "for column_name in ['allocataire-date_naissance', 'allocataire-pays_naissance',\n",
    "       'allocataire-commune_naissance', 'allocataire-code_iso_pays_naissance',\n",
    "       'allocataire-code_insee_commune_naissance']:\n",
    "    if column_name not in df_existing_db_unwrapped_alloc.columns:\n",
    "        df_existing_db_unwrapped_alloc[column_name] = np.NaN\n",
    "\n",
    "\n",
    "# remove duplicate rows (use all relevant columns meaning all minus id_psp, created_at, update_at)\n",
    "## This implies :\n",
    "### - Remove these rows from merging\n",
    "### - No update on these rows, we just keep them in DB\n",
    "### - No email communication on these row => We don't want the same beneficiary to receiv 2 differents code\n",
    "df_existing_unwrapped_no_duplicate = df_existing_db_unwrapped_alloc.drop_duplicates(subset=['nom', 'prenom', 'genre', 'organisme', 'situation', \n",
    "'allocataire-qualite',\n",
    "'allocataire-matricule',\n",
    "'allocataire-code_organisme',\n",
    "'allocataire-telephone',\n",
    "'allocataire-nom',\n",
    "'allocataire-prenom',\n",
    "'allocataire-date_naissance',\n",
    "'allocataire-courriel',\n",
    "'allocataire-code_insee_commune_naissance',\n",
    "'allocataire-commune_naissance',\n",
    "'allocataire-code_iso_pays_naissance',\n",
    "'allocataire-pays_naissance'\n",
    "])\n",
    "\n",
    "print(f\"{len(df_existing_db_unwrapped_alloc) - len(df_existing_unwrapped_no_duplicate)} duplicate rows where removed based on all columns\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release\n",
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_existing_db_unwrapped_alloc\n",
    "    del mask_9_char_phone\n",
    "    del mask_no_zero_phone_number\n",
    "    del mask_tel_not_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading new data\n",
    "df_new_db = pd.read_csv(new_db_export_filepath, index_col=0, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "df_new_db['date_naissance'] = pd.to_datetime(df_new_db['date_naissance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json unwrap\n",
    "df_json_allocataire_new = pd.json_normalize(df_new_db['allocataire'].apply(json.loads))\n",
    "df_json_allocataire_new = df_json_allocataire_new.add_prefix('allocataire-')\n",
    "df_new_db.index = pd.RangeIndex(start=0, stop=len(df_new_db), step=1)\n",
    "df_new_db_unwrapped_alloc = pd.merge(df_new_db, df_json_allocataire_new, left_index=True, right_index=True)\n",
    "df_new_db_unwrapped_alloc = df_new_db_unwrapped_alloc.drop(columns=['allocataire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_new_db\n",
    "    del df_json_allocataire_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data casting and formating\n",
    "df_new_db_unwrapped_alloc['nom'] = df_new_db_unwrapped_alloc['nom'].str.upper()\n",
    "df_new_db_unwrapped_alloc['prenom'] = df_new_db_unwrapped_alloc['prenom'].str.upper()\n",
    "\n",
    "mask_matricule_not_null = df_new_db_unwrapped_alloc['allocataire-matricule'].notna()\n",
    "df_new_db_unwrapped_alloc['allocataire-matricule'] = df_new_db_unwrapped_alloc.loc[mask_matricule_not_null, 'allocataire-matricule'].astype(str)\n",
    "\n",
    "df_new_db_unwrapped_alloc['allocataire-code_organisme'] = df_new_db_unwrapped_alloc['allocataire-code_organisme'].fillna(0)\n",
    "df_new_db_unwrapped_alloc['allocataire-code_organisme'] = df_new_db_unwrapped_alloc['allocataire-code_organisme'].astype(int)\n",
    "# mask_code_orgnisme_not_null = df_new_db_unwrapped_alloc['allocataire-code_organisme'].notna()\n",
    "# # df_new_db_unwrapped_alloc['allocataire-code_organisme'] =\n",
    "# TODO : Fix problème ici\n",
    "# df_new_db_unwrapped_alloc['allocataire-code_organisme'] = df_new_db_unwrapped_alloc['allocataire-code_organisme'].replace(0, np.nan)\n",
    "\n",
    "# replace blank string with nan\n",
    "df_new_db_unwrapped_alloc = df_new_db_unwrapped_alloc.replace(r'', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del mask_matricule_not_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward and forward fill on all columns group to ease merging\n",
    "def forward_and_backward_fill(group):\n",
    "    return group.ffill().bfill().iloc[-1]\n",
    "\n",
    "df_subset_bf_filled = df_existing_unwrapped_no_duplicate.groupby(['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre']) \\\n",
    "                                                 .filter(lambda x: len(x) > 1) \\\n",
    "                                                 .groupby(['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre']) \\\n",
    "                                                 .apply(lambda group: group.ffill().bfill()) \\\n",
    "                                                 .reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-apply remove duplicate (ignore code and adresse)\n",
    "df_subset_duplicates_merged = df_subset_bf_filled.drop_duplicates(subset=['nom', 'prenom', 'genre', 'organisme', 'situation', \n",
    "'allocataire-qualite',\n",
    "'allocataire-matricule',\n",
    "'allocataire-code_organisme',\n",
    "'allocataire-telephone',\n",
    "'allocataire-nom',\n",
    "'allocataire-prenom',\n",
    "'allocataire-date_naissance',\n",
    "'allocataire-courriel',\n",
    "'allocataire-code_insee_commune_naissance',\n",
    "'allocataire-commune_naissance',\n",
    "'allocataire-code_iso_pays_naissance',\n",
    "'allocataire-pays_naissance'\n",
    "])\n",
    "\n",
    "print(f\"{len(df_subset_bf_filled) - len(df_subset_duplicates_merged)} duplicate rows where removed based on all columns after forward and backward fills\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_subset_bf_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep latest update date on existings\n",
    "df_subset_duplicated_final = df_subset_duplicates_merged.sort_values('updated_at').drop_duplicates(subset=['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'], keep='last')\n",
    "\n",
    "print(f\"{len(df_subset_duplicates_merged) - len(df_subset_duplicated_final)} duplicate rows where removed based on nom, prenom, matricule, date_naissance and genre\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_subset_duplicates_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve other part \n",
    "df_subset_single = df_existing_unwrapped_no_duplicate.groupby(['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre']) \\\n",
    "                                                 .filter(lambda x: len(x) == 1)\n",
    "\n",
    "\n",
    "print(f\"{len(df_subset_single)} rows are found not duplicated based on 'nom', 'prenom', 'matricule', 'date_naissance' and 'genre'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging both on existing datas\n",
    "df_exi_unwrapped_no_duplicate = pd.concat([df_subset_single, df_subset_duplicated_final], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_subset_single\n",
    "    del df_subset_duplicated_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we wan't to ensure no duplicate beneficiary on merge\n",
    "df_new_duplicate = df_exi_unwrapped_no_duplicate.groupby(['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre']).filter(lambda x: len(x) > 1)\n",
    "assert len(df_new_duplicate) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we wan't to ensure no merge keys are unique in both dataframe\n",
    "df_exi_duplicate = df_exi_unwrapped_no_duplicate.groupby(['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre']).filter(lambda x: len(x) > 1)\n",
    "assert len(df_exi_duplicate) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_exi_duplicate\n",
    "    del df_new_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_db_unwrapped_alloc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove this after re executing step 1\n",
    "# del df_new_db_unwrapped_alloc['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add existing id_psp code and created_at to new data when possible\n",
    "df_new_unwrapped_no_duplicate_with_id = df_new_db_unwrapped_alloc.merge(\\\n",
    "            df_exi_unwrapped_no_duplicate[['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre', 'id_psp', 'created_at']], \\\n",
    "            on=['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'], \\\n",
    "            suffixes=(None,'_new'), \\\n",
    "            how='left')\n",
    "mask_has_code = ~df_new_unwrapped_no_duplicate_with_id['id_psp'].isna()\n",
    "print(f\"{len(df_new_unwrapped_no_duplicate_with_id[mask_has_code])} rows already have a code, created_at is copied on them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del mask_has_code\n",
    "    del df_new_db_unwrapped_alloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on concat pour former en thérorie des paires de doublons dans l'optique de supprimer les duplicats entre les nouvelles et les anciennes données\n",
    "df_concatenated = pd.concat([df_new_unwrapped_no_duplicate_with_id, df_exi_unwrapped_no_duplicate], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_new_unwrapped_no_duplicate_with_id\n",
    "    del df_exi_unwrapped_no_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_create_at(df_group):\n",
    "#     df_group.iloc[0, df_group.columns.get_loc('created_at')] = df_group.iloc[1]['created_at']\n",
    "#     return df_group\n",
    "\n",
    "# # save index has a new column\n",
    "# # group again has filter return a plain df\n",
    "# # asign oldest created_at to every rows in group \n",
    "# # remove apply multi-index\n",
    "# # set index column as new index\n",
    "# df_temp = df_concatenated.groupby(['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'])\\\n",
    "#                                 .filter(lambda x: len(x) > 1)\\\n",
    "#                                 .reset_index()\\\n",
    "#                                 .groupby(['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'],)\\\n",
    "#                                 .apply(lambda df_group: update_create_at(df_group), include_groups = False)\\\n",
    "#                                 .reset_index()\\\n",
    "#                                 .set_index('index')\n",
    "# df_temp.index.name = None\n",
    "\n",
    "# print(f\"{len(df_temp)} duplicated rows based on 'nom', 'prenom', 'matricule', 'date_naissance' and 'genre' should keep oldest created_at\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assignation based on index\n",
    "# df_concatenated.loc[df_temp.index] = df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_concatenated.groupby(['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'])\\\n",
    "#                                 .filter(lambda x: len(x) > 1)\\\n",
    "#                                 .reset_index()\\\n",
    "#                                 .sort_values(['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate, new data prevails\n",
    "df_concatenated_no_duplicates = df_concatenated.drop_duplicates(subset=['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'], keep='first')\n",
    "\n",
    "print(f\"{len(df_concatenated) - len(df_concatenated_no_duplicates)} duplicated rows deleted based on 'nom', 'prenom', 'matricule', 'date_naissance' and 'genre', latest data are kept\")\n",
    "# TODO: (optional) we could add a step here to ensure data from existing are kept if no value are present in the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure no duplicate beneficiary \n",
    "df_concatenated_only_more_than_2 = df_concatenated_no_duplicates.groupby(['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre']).filter(lambda x: len(x) > 1)\n",
    "assert len(df_concatenated_only_more_than_2) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_concatenated_only_more_than_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve those without code\n",
    "mask_no_code = df_concatenated_no_duplicates['id_psp'].isna()\n",
    "df_no_code = df_concatenated_no_duplicates[mask_no_code]\n",
    "\n",
    "print(f\"{len(df_no_code)} rows without code\")\n",
    "print(f\"{len(df_concatenated_no_duplicates[~mask_no_code])} rows with code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new code ensuring no duplicates with existings\n",
    "import random\n",
    "import string\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now()\n",
    "current_year = str(current_date.year)[-2:]\n",
    "\n",
    "def get_characters_set(size = 4):\n",
    "    return ''.join(random.choices([c for c in string.ascii_uppercase if c not in 'OI'], k=size))\n",
    "    \n",
    "def generate_code():\n",
    "    return f\"{current_year}-{get_characters_set(4)}-{get_characters_set(4)}\"\n",
    "\n",
    "# init set of codes with existing\n",
    "unique_codes = set(df_concatenated_no_duplicates[~mask_no_code]['id_psp'])\n",
    "\n",
    "# init current_code count\n",
    "current_codes_count = len(unique_codes)\n",
    "while len(unique_codes) < current_codes_count + len(df_no_code):\n",
    "    code = generate_code()\n",
    "    unique_codes.add(code)\n",
    "\n",
    "# only retrieve newly created codes\n",
    "new_codes = list(unique_codes.difference(df_concatenated_no_duplicates['id_psp']))\n",
    "df_new_codes = pd.DataFrame({'id_psp': new_codes})\n",
    "\n",
    "print(f\"{len(df_new_codes)} generated codes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with no_code dataframe\n",
    "df_final_with_new_code = df_no_code.reset_index().combine_first(df_new_codes.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION:\n",
    "    del df_no_code\n",
    "    del df_new_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat with already existing codes dataframe \n",
    "mask_has_code = ~df_concatenated_no_duplicates['id_psp'].isna()\n",
    "df_has_code = df_concatenated_no_duplicates[mask_has_code]\n",
    "df_final_with_code = pd.concat([df_has_code, df_final_with_new_code], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION:\n",
    "    del df_has_code\n",
    "    del df_final_with_new_code\n",
    "    del df_concatenated_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure everyone has a code\n",
    "mask_no_code = df_final_with_code['id_psp'].isna()\n",
    "df_still_no_code = df_final_with_code[mask_no_code]\n",
    "assert len(df_still_no_code) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION:\n",
    "    del df_still_no_code\n",
    "    del mask_no_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_with_code = df_final_with_code.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "# add created_at for new rows\n",
    "# using time zone for created_at to be iso existings\n",
    "tz = pytz.timezone('Europe/Paris')\n",
    "now = datetime.datetime.now()\n",
    "now_tz = tz.localize(now)\n",
    "\n",
    "mask_no_created_at = df_final_with_code['created_at'].isna()\n",
    "\n",
    "df_final_with_code.loc[mask_no_created_at, 'created_at'] = now_tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map to json values for target DB model \n",
    "df_final_with_code['allocataire'] = df_final_with_code.apply(lambda row: json.dumps ({\n",
    "    'qualite': row['allocataire-qualite'],\n",
    "    'matricule': row['allocataire-matricule'],\n",
    "    'code_organisme': row['allocataire-code_organisme'],\n",
    "    'telephone': row['allocataire-telephone'],\n",
    "    'nom': row['allocataire-nom'],\n",
    "    'prenom': row['allocataire-prenom'],\n",
    "    'date_naissance': row['allocataire-date_naissance'],\n",
    "    'courriel': row['allocataire-courriel'],\n",
    "    'code_insee_commune_naissance': row['allocataire-code_insee_commune_naissance'],\n",
    "    'commune_naissance': row['allocataire-commune_naissance'],\n",
    "    'code_iso_pays_naissance': row['allocataire-code_iso_pays_naissance'],\n",
    "    'pays_naissance': row['allocataire-pays_naissance'],\n",
    "}), axis=1)\n",
    "\n",
    "# we don't do update adresse so we don't unwrap it at the moment\n",
    "# def to_json_adresse_without_null(row):\n",
    "#     adresse_mapping = {\n",
    "#         'voie': row['adresse_allocataire-voie'],\n",
    "#         'code_postal': row['adresse_allocataire-code_postal'],\n",
    "#         'nom_adresse_postale': row['adresse_allocataire-nom_adresse_postale'],\n",
    "#         'commune': row['adresse_allocataire-commune'],\n",
    "#         'code_insee': row['adresse_allocataire-code_insee'],\n",
    "#         'cplt_adresse': row['adresse_allocataire-cplt_adresse'],\n",
    "#     }\n",
    "#     filtered_address = {k: v for k, v in adresse_mapping.items() if pd.notnull(v)}\n",
    "#     return json.dumps(filtered_address)\n",
    "\n",
    "\n",
    "\n",
    "# df_final_with_new_code['adresse_allocataire'] = df_final_with_new_code.apply(to_json_adresse_without_null, axis=1)\n",
    "\n",
    "df_final_with_code = df_final_with_code.drop(columns=[\n",
    "'allocataire-qualite',\n",
    "'allocataire-matricule',\n",
    "'allocataire-code_organisme',\n",
    "'allocataire-nom',\n",
    "'allocataire-prenom',\n",
    "'allocataire-telephone',\n",
    "'allocataire-date_naissance',\n",
    "'allocataire-courriel',\n",
    "'allocataire-code_insee_commune_naissance',\n",
    "'allocataire-commune_naissance',\n",
    "'allocataire-code_iso_pays_naissance',\n",
    "'allocataire-pays_naissance',\n",
    "# 'adresse_allocataire-voie',\n",
    "# 'adresse_allocataire-nom_adresse_postale',\n",
    "# 'adresse_allocataire-code_postal',\n",
    "# 'adresse_allocataire-commune',\n",
    "# 'adresse_allocataire-code_insee',\n",
    "# 'adresse_allocataire-cplt_adresse',\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to CSV\n",
    "df_final_with_code.to_csv(concatenated_db_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_analyse = df_merged_on_multi_criteria[[\n",
    "#     'nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre',\n",
    "\n",
    "#     'allocataire-qualite_x',\n",
    "#     # 'allocataire-code_organisme_x',\n",
    "#     'allocataire-telephone_x',\n",
    "#     'allocataire-nom_x',\n",
    "#     'allocataire-prenom_x',\n",
    "#     'allocataire-date_naissance',\n",
    "#     'allocataire-courriel_x',\n",
    "#     # 'allocataire-code_insee_commune_naissance_x',\n",
    "#     # 'allocataire-commune_naissance_x',\n",
    "#     # 'allocataire-code_iso_pays_naissance_x',\n",
    "#     # 'allocataire-pays_naissance_x',\n",
    "    \n",
    "#     'allocataire-qualite_y',\n",
    "#     # 'allocataire-code_organisme_y',\n",
    "#     'allocataire-telephone_y',\n",
    "#     'allocataire-nom_y',\n",
    "#     'allocataire-prenom_y',\n",
    "#     'allocataire-dateNaissance',\n",
    "#     'allocataire-courriel_y'\n",
    "#     #'allocataire-code_insee_commune_naissance_y',\n",
    "#     #'allocataire-commune_naissance_y',\n",
    "#     #'allocataire-code_iso_pays_naissance_y',\n",
    "#     #'allocataire-pays_naissance_y'\n",
    "#     ]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
