{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération des codes et merge avec la BDD existante\n",
    "\n",
    "## Traitements\n",
    "\n",
    "\n",
    "1. Chargement de la bdd existante et de la nouvelle généré à l'étape 1 (au même format)\n",
    "2. Merge des données sur matricule, nom, prénom, date de naissance du bénéficiaire (INNER)\n",
    "3. On garde les nouvelles données pour le résultat du merge\n",
    "3. Ajout des nouvelles données qui n'étaient pas présentes\n",
    "4. Génération des codes manquants pour les nouvelles lignes de bénéficiaires\n",
    "5. Output to csv\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "\n",
    "Création des bénéficiaires manuellement à partir du 27 Juin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T08:05:49.062431Z",
     "start_time": "2024-08-01T08:05:49.052576Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "existing_db_export_filepath = os.environ['EXISTING_DB']\n",
    "new_db_export_filepath = os.environ['DB_EXPORT']\n",
    "concatenated_db_filepath = os.environ['DB_CONCATENATED']\n",
    "final_merged_with_ids = os.environ['DB_MERGED_WITH_ID']\n",
    "final_merged_without_ids = os.environ['DB_MERGED_WITHOUT_ID']\n",
    "\n",
    "MEMORY_OPTIMIZATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_type = {\n",
    "  'qpv': 'boolean',\n",
    "  'a_valider': 'boolean',\n",
    "  'zrr': 'boolean',\n",
    "}\n",
    "\n",
    "df_existing_db = pd.read_csv(existing_db_export_filepath, sep=',',  dtype=column_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to date for comparison, timestamp is added back later down the line\n",
    "df_existing_db['date_naissance'] = pd.to_datetime(df_existing_db['date_naissance']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the beneficiaires that have been manually added and need verifications internally to avoid injection conflict\n",
    "df_existing_db = df_existing_db[df_existing_db['a_valider'] != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_allocataire = pd.json_normalize(df_existing_db['allocataire'].apply(json.loads))\n",
    "df_json_allocataire = df_json_allocataire.add_prefix('allocataire-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing_db.index = pd.RangeIndex(start=0, stop=len(df_existing_db), step=1)\n",
    "df_existing_db_unwrapped_alloc = pd.merge(df_existing_db, df_json_allocataire, left_index=True, right_index=True)\n",
    "df_existing_db_unwrapped_alloc = df_existing_db_unwrapped_alloc.drop(columns=['allocataire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_existing_db\n",
    "    del df_json_allocataire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data casting and formating\n",
    "df_existing_db_unwrapped_alloc['allocataire-matricule'] = df_existing_db_unwrapped_alloc['allocataire-matricule'].astype(str)\n",
    "df_existing_db_unwrapped_alloc['allocataire-nom'] = df_existing_db_unwrapped_alloc['allocataire-nom'].str.upper()\n",
    "df_existing_db_unwrapped_alloc['allocataire-prenom'] = df_existing_db_unwrapped_alloc['allocataire-prenom'].str.upper()\n",
    "\n",
    "# cleaning (telephone)\n",
    "df_existing_db_unwrapped_alloc['allocataire-telephone'] = df_existing_db_unwrapped_alloc['allocataire-telephone'].replace('0', '')\n",
    "mask_tel_not_null = df_existing_db_unwrapped_alloc['allocataire-telephone'].notna()\n",
    "mask_no_zero_phone_number = ~df_existing_db_unwrapped_alloc.loc[mask_tel_not_null, 'allocataire-telephone'].str.startswith('0')\n",
    "mask_9_char_phone = df_existing_db_unwrapped_alloc.loc[mask_tel_not_null, 'allocataire-telephone'].str.len() == 9\n",
    "df_existing_db_unwrapped_alloc.loc[mask_tel_not_null & mask_no_zero_phone_number & mask_9_char_phone, 'allocataire-telephone'] = '0' + df_existing_db_unwrapped_alloc['allocataire-telephone']\n",
    "\n",
    "# replace blank string with nan\n",
    "df_existing_db_unwrapped_alloc = df_existing_db_unwrapped_alloc.replace(r'', np.NaN)\n",
    "df_existing_db_unwrapped_alloc = df_existing_db_unwrapped_alloc.replace(r'0', np.NaN)\n",
    "df_existing_db_unwrapped_alloc = df_existing_db_unwrapped_alloc.replace(r'00000', np.NaN)\n",
    "\n",
    "# lower case on email\n",
    "df_existing_db_unwrapped_alloc['allocataire-courriel'] = df_existing_db_unwrapped_alloc['allocataire-courriel'].str.lower()\n",
    "\n",
    "# check all columns exists, create them otherwise (happens if we act on CNOUS only data)\n",
    "for column_name in ['allocataire-date_naissance', 'allocataire-pays_naissance',\n",
    "       'allocataire-commune_naissance', 'allocataire-code_iso_pays_naissance',\n",
    "       'allocataire-code_insee_commune_naissance']:\n",
    "    if column_name not in df_existing_db_unwrapped_alloc.columns:\n",
    "        df_existing_db_unwrapped_alloc[column_name] = np.NaN\n",
    "\n",
    "\n",
    "# remove duplicate rows (use all relevant columns meaning all minus id_psp, created_at, update_at)\n",
    "## This implies :\n",
    "### - Remove these rows from merging\n",
    "### - No update on these rows, we just keep them in DB\n",
    "### - No email communication on these row => We don't want the same beneficiary to receive 2 differents code\n",
    "df_existing_unwrapped_no_duplicate = df_existing_db_unwrapped_alloc.drop_duplicates(subset=[\n",
    "  'nom', \n",
    "  'prenom', \n",
    "  'genre',\n",
    "  'organisme',\n",
    "  'situation', \n",
    "  'allocataire-qualite',\n",
    "  'allocataire-matricule',\n",
    "  'allocataire-code_organisme',\n",
    "  'allocataire-telephone',\n",
    "  'allocataire-nom',\n",
    "  'allocataire-prenom',\n",
    "  'allocataire-date_naissance',\n",
    "  'allocataire-courriel',\n",
    "  'allocataire-code_insee_commune_naissance',\n",
    "  'allocataire-commune_naissance',\n",
    "  'allocataire-code_iso_pays_naissance',\n",
    "  'allocataire-pays_naissance'\n",
    "])\n",
    "\n",
    "print(f\"{len(df_existing_db_unwrapped_alloc) - len(df_existing_unwrapped_no_duplicate)} duplicate rows where removed based on all columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release\n",
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_existing_db_unwrapped_alloc\n",
    "    del mask_9_char_phone\n",
    "    del mask_no_zero_phone_number\n",
    "    del mask_tel_not_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading new data\n",
    "df_new_db = pd.read_csv(new_db_export_filepath, index_col=0, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to date for comparison, timestamp is added back later down the line\n",
    "df_new_db['date_naissance'] = pd.to_datetime(df_new_db['date_naissance']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json unwrap allocatire\n",
    "df_json_allocataire_new = pd.json_normalize(df_new_db['allocataire'].apply(json.loads))\n",
    "df_json_allocataire_new = df_json_allocataire_new.add_prefix('allocataire-')\n",
    "\n",
    "df_new_db.index = pd.RangeIndex(start=0, stop=len(df_new_db), step=1)\n",
    "df_new_db_unwrapped_alloc = pd.merge(df_new_db, df_json_allocataire_new, left_index=True, right_index=True)\n",
    "\n",
    "df_new_db_unwrapped_alloc = df_new_db_unwrapped_alloc.drop(columns=['allocataire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_new_db\n",
    "    del df_json_allocataire_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data casting and formating\n",
    "df_new_db_unwrapped_alloc['nom'] = df_new_db_unwrapped_alloc['nom'].str.upper()\n",
    "df_new_db_unwrapped_alloc['prenom'] = df_new_db_unwrapped_alloc['prenom'].str.upper()\n",
    "\n",
    "mask_matricule_not_null = df_new_db_unwrapped_alloc['allocataire-matricule'].notna()\n",
    "df_new_db_unwrapped_alloc['allocataire-matricule'] = df_new_db_unwrapped_alloc.loc[mask_matricule_not_null, 'allocataire-matricule'].astype(str)\n",
    "\n",
    "df_new_db_unwrapped_alloc['allocataire-code_organisme'] = df_new_db_unwrapped_alloc['allocataire-code_organisme'].fillna(0)\n",
    "df_new_db_unwrapped_alloc['allocataire-code_organisme'] = df_new_db_unwrapped_alloc['allocataire-code_organisme'].astype(int)\n",
    "\n",
    "# replace blank string with nan\n",
    "df_new_db_unwrapped_alloc = df_new_db_unwrapped_alloc.replace(r'', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del mask_matricule_not_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward and forward fill on all columns group to ease merging\n",
    "def forward_and_backward_fill(group):\n",
    "    return group.ffill().bfill().iloc[-1]\n",
    "\n",
    "df_subset_bf_filled = df_existing_unwrapped_no_duplicate.groupby(['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre']) \\\n",
    "                                                 .filter(lambda x: len(x) > 1) \\\n",
    "                                                 .groupby(['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre']) \\\n",
    "                                                 .apply(lambda group: group.ffill().bfill()) \\\n",
    "                                                 .reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-apply remove duplicate (ignore code and adresse)\n",
    "df_subset_duplicates_merged = df_subset_bf_filled.drop_duplicates(subset=[\n",
    "  'nom',\n",
    "  'prenom',\n",
    "  'genre',\n",
    "  'organisme',\n",
    "  'situation', \n",
    "  'allocataire-qualite',\n",
    "  'allocataire-matricule',\n",
    "  'allocataire-code_organisme',\n",
    "  'allocataire-telephone',\n",
    "  'allocataire-nom',\n",
    "  'allocataire-prenom',\n",
    "  'allocataire-date_naissance',\n",
    "  'allocataire-courriel',\n",
    "  'allocataire-code_insee_commune_naissance',\n",
    "  'allocataire-commune_naissance',\n",
    "  'allocataire-code_iso_pays_naissance',\n",
    "  'allocataire-pays_naissance'\n",
    "])\n",
    "\n",
    "print(f\"{len(df_subset_bf_filled) - len(df_subset_duplicates_merged)} duplicate rows were removed based on all columns after forward and backward fills\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_subset_bf_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep latest update date on existing\n",
    "df_subset_duplicated_final = df_subset_duplicates_merged.sort_values('updated_at').drop_duplicates(subset=['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'], keep='last')\n",
    "\n",
    "print(f\"{len(df_subset_duplicates_merged) - len(df_subset_duplicated_final)} duplicate rows where removed based on nom, prenom, matricule, date_naissance and genre\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_subset_duplicates_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve other part\n",
    "duplicates_from_existing = df_existing_unwrapped_no_duplicate.duplicated(subset=['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'], keep=False)\n",
    "\n",
    "# # Keep rows that are not duplicated\n",
    "df_subset_single = df_existing_unwrapped_no_duplicate[~duplicates_from_existing]\n",
    "\n",
    "print(f\"{len(df_subset_single)} rows are found not duplicated based on 'nom', 'prenom', 'matricule', 'date_naissance' and 'genre'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging both on existing datas\n",
    "df_exi_unwrapped_no_duplicate = pd.concat([df_subset_single, df_subset_duplicated_final], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del duplicates_from_existing\n",
    "    del df_subset_single\n",
    "    del df_subset_duplicated_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to ensure no duplicate on existing beneficiary on merge\n",
    "# Identify duplicates based on the specified columns\n",
    "duplicates_from_existing = df_exi_unwrapped_no_duplicate.duplicated(subset=['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'], keep=False)\n",
    "\n",
    "# Filter the dataframe to get only duplicate rows\n",
    "df_duplicates_from_existing = df_exi_unwrapped_no_duplicate[duplicates_from_existing]\n",
    "\n",
    "# Ensure there are no duplicates in the original dataframe\n",
    "assert len(df_duplicates_from_existing) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to ensure no duplicate on new\n",
    "# Identify duplicates based on the specified columns\n",
    "duplicates_from_new = df_new_db_unwrapped_alloc.duplicated(subset=['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'], keep=False)\n",
    "\n",
    "# Filter the dataframe to get only duplicate rows\n",
    "df_duplicates_from_new = df_new_db_unwrapped_alloc[duplicates_from_new]\n",
    "\n",
    "# Ensure there are no duplicates in the original dataframe\n",
    "assert len(df_duplicates_from_existing) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_duplicates_from_existing\n",
    "    del df_duplicates_from_new\n",
    "    del duplicates_from_existing\n",
    "    del duplicates_from_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add existing id_psp code and created_at to new data when possible\n",
    "df_new_unwrapped_no_duplicate_with_id = df_new_db_unwrapped_alloc.merge(\\\n",
    "            df_exi_unwrapped_no_duplicate[['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre', 'id_psp', 'created_at', 'id', 'qpv', 'zrr', 'a_valider']], \\\n",
    "            on=['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'], \\\n",
    "            suffixes=(None,'_new'), \\\n",
    "            how='left')\n",
    "\n",
    "mask_has_code = ~df_new_unwrapped_no_duplicate_with_id['id_psp'].isna()\n",
    "\n",
    "print(f\"{len(df_new_unwrapped_no_duplicate_with_id[mask_has_code])} rows already have a code, created_at is copied on them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del mask_has_code\n",
    "    del df_new_db_unwrapped_alloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on concat pour former en théorie des paires de doublons dans l'optique de supprimer les duplicats entre les nouvelles et les anciennes données\n",
    "df_concatenated = pd.concat([df_new_unwrapped_no_duplicate_with_id, df_exi_unwrapped_no_duplicate], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map 'jeune' to 'Jeune'\n",
    "df_concatenated['situation'] = df_concatenated['situation'].replace('jeune', 'Jeune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION: \n",
    "    del df_new_unwrapped_no_duplicate_with_id\n",
    "    del df_exi_unwrapped_no_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate, new data prevails\n",
    "df_concatenated_no_duplicates = df_concatenated.drop_duplicates(subset=['nom', 'prenom', 'allocataire-matricule', 'date_naissance', 'genre'], keep='first')\n",
    "\n",
    "print(f\"{len(df_concatenated) - len(df_concatenated_no_duplicates)} duplicated rows deleted based on 'nom', 'prenom', 'matricule', 'date_naissance' and 'genre', latest data are kept\")\n",
    "# TODO: (optional) we could add a step here to ensure data from existing are kept if no value are present in the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve those without code\n",
    "mask_no_code = df_concatenated_no_duplicates['id_psp'].isna()\n",
    "df_no_code = df_concatenated_no_duplicates.loc[mask_no_code]\n",
    "\n",
    "print(f\"{len(df_no_code)} rows without code\")\n",
    "print(f\"{len(df_concatenated_no_duplicates[~mask_no_code])} rows with code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new code ensuring no duplicates with existings\n",
    "import random\n",
    "import string\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now()\n",
    "current_year = str(current_date.year)[-2:]\n",
    "\n",
    "def get_characters_set(size = 4):\n",
    "    return ''.join(random.choices([c for c in string.ascii_uppercase if c not in 'OI'], k=size))\n",
    "    \n",
    "def generate_code():\n",
    "    return f\"{current_year}-{get_characters_set(4)}-{get_characters_set(4)}\"\n",
    "\n",
    "# init set of codes with existing\n",
    "unique_codes = set(df_concatenated_no_duplicates[~mask_no_code]['id_psp'])\n",
    "\n",
    "# init current_code count\n",
    "current_codes_count = len(unique_codes)\n",
    "while len(unique_codes) < current_codes_count + len(df_no_code):\n",
    "    code = generate_code()\n",
    "    unique_codes.add(code)\n",
    "\n",
    "# only retrieve newly created codes\n",
    "new_codes = list(unique_codes.difference(df_concatenated_no_duplicates['id_psp']))\n",
    "df_new_codes = pd.DataFrame({'id_psp': new_codes})\n",
    "\n",
    "print(f\"{len(df_new_codes)} generated codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with no_code dataframe\n",
    "df_final_with_new_code = df_no_code.reset_index(drop=True).combine_first(df_new_codes.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION:\n",
    "    del df_no_code\n",
    "    del df_new_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat with already existing codes dataframe \n",
    "mask_has_code = ~df_concatenated_no_duplicates['id_psp'].isna()\n",
    "df_has_code = df_concatenated_no_duplicates[mask_has_code]\n",
    "df_final_with_code = pd.concat([df_has_code, df_final_with_new_code], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION:\n",
    "    del df_has_code\n",
    "    del df_concatenated_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure everyone has a code\n",
    "mask_no_code = df_final_with_code['id_psp'].isna()\n",
    "df_still_no_code = df_final_with_code[mask_no_code]\n",
    "assert len(df_still_no_code) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MEMORY_OPTIMIZATION:\n",
    "    del df_still_no_code\n",
    "    del mask_no_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_with_code = df_final_with_code.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "# add created_at for new rows\n",
    "# using time zone for created_at to be iso existings\n",
    "tz = pytz.timezone('Europe/Paris')\n",
    "now = datetime.datetime.now()\n",
    "now_tz = tz.localize(now)\n",
    "\n",
    "mask_no_created_at = df_final_with_code['created_at'].isna()\n",
    "\n",
    "df_final_with_code.loc[mask_no_created_at, 'created_at'] = now_tz\n",
    "df_final_with_code[['zrr', 'qpv', 'a_valider']].fillna(value=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map to json values for target DB model \n",
    "def to_json_allocataire_without_null(row):\n",
    "    allocataire_mapping = {\n",
    "        'qualite': row['allocataire-qualite'],\n",
    "        'matricule': row['allocataire-matricule'],\n",
    "        'code_organisme': row['allocataire-code_organisme'],\n",
    "        'telephone': row['allocataire-telephone'],\n",
    "        'nom': row['allocataire-nom'],\n",
    "        'prenom': row['allocataire-prenom'],\n",
    "        'date_naissance': row['allocataire-date_naissance'],\n",
    "        'courriel': row['allocataire-courriel'],\n",
    "        'code_insee_commune_naissance': row['allocataire-code_insee_commune_naissance'],\n",
    "        'commune_naissance': row['allocataire-commune_naissance'],\n",
    "        'code_iso_pays_naissance': row['allocataire-code_iso_pays_naissance'],\n",
    "        'pays_naissance': row['allocataire-pays_naissance']\n",
    "    }\n",
    "    filtered_nan_allocataire = {k: v for k, v in allocataire_mapping.items() if pd.notnull(v)}\n",
    "    return json.dumps(filtered_nan_allocataire, ensure_ascii=False)\n",
    "\n",
    "df_final_with_code['allocataire'] = df_final_with_code.apply(to_json_allocataire_without_null, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_with_code.loc[df_final_with_code['refuser'].isna(), 'refuser'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_with_code = df_final_with_code.drop(columns=[\n",
    "  'allocataire-qualite',\n",
    "  'allocataire-matricule',\n",
    "  'allocataire-code_organisme',\n",
    "  'allocataire-nom',\n",
    "  'allocataire-prenom',\n",
    "  'allocataire-telephone',\n",
    "  'allocataire-date_naissance',\n",
    "  'allocataire-courriel',\n",
    "  'allocataire-code_insee_commune_naissance',\n",
    "  'allocataire-commune_naissance',\n",
    "  'allocataire-code_iso_pays_naissance',\n",
    "  'allocataire-pays_naissance',\n",
    "  'allocataire-departement_naissance'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add back timestamp and the 4 hours to be iso with the existing database\n",
    "df_final_with_code['date_naissance'] = pd.to_datetime(df_final_with_code['date_naissance']).dt.floor('D') + pd.DateOffset(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take existing\n",
    "df_final_existing_with_codes = df_final_with_code[~df_final_with_code['id'].isna()]\n",
    "len(df_final_existing_with_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take new\n",
    "df_final_existing_without_codes = df_final_with_code[df_final_with_code['id'].isna()]\n",
    "len(df_final_existing_without_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_existing_with_codes.to_csv(final_merged_with_ids, index=False)\n",
    "df_final_existing_without_codes.to_csv(final_merged_without_ids, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
