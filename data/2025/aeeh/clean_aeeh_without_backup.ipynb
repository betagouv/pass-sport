{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "- Load backup file\n",
    "- Load DS (Demarches Simplifiees) CSV file\n",
    "- Apply eligibility dates\n",
    "  - 01/01/2006 to 31/12/2019 (inclusives)\n",
    "- Clean & Format the rows\n",
    "  - Add the column \"folder_number\" (default to np.NaN)\n",
    "  - Remove duplicate from GRIST AEEH list\n",
    "- Output 1 CSV file with the database format (to be injection ready)\n",
    "- Output 1 CSV file for the support team with the created column and the \"folder_number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from utils.data_utils import unaccent_and_upper, format_insee_or_postal_code, get_current_date_for_file_name\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ds_input_filepath = os.environ['DEMARCHES_SIMPLIFIEES_PATHFILE_2025']\n",
    "existing_codes_filepath = os.environ['EXISTING_CODES_PATHFILE_2025']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossiers_columns = ['dossier_id', 'prenom', 'nom', 'date_naissance', 'allocataire-courriel']\n",
    "db_columns = ['nom', 'prenom', 'date_naissance', 'genre', 'organisme', 'situation', 'allocataire', 'adresse_allocataire', 'created_at', 'updated_at', 'exercice_id', 'uuid_doc', 'zrr', 'qpv', 'a_valider', 'refuser', 'id_psp']\n",
    "db_columns_with_dossier = ['dossier_id', 'nom', 'prenom', 'date_naissance', 'genre', 'organisme', 'situation', 'allocataire', 'adresse_allocataire', 'created_at', 'updated_at', 'exercice_id', 'uuid_doc', 'zrr', 'qpv', 'a_valider', 'refuser', 'id_psp']\n",
    "\n",
    "column_mapping = {\n",
    "  \"ID\" : \"dossier_id\",\n",
    "  \"Email\": \"demandeur_email\",\n",
    "  \"FranceConnect ?\": \"france_connect_a_ete_utilise\",\n",
    "  \"Civilité\": \"qualite\",\n",
    "  \"Nom\": \"demandeur_nom\",\n",
    "  \"Prénom\": \"demandeur_prenom\",\n",
    "  \"Dépôt pour un tiers\": \"depot_pour_un_tiers\",\n",
    "  \"Nom du mandataire\": \"nom_mandataire\",\n",
    "  \"Prénom du mandataire\": \"prenom_mandataire\",\n",
    "  \"À archiver\": \"a_archiver\",\n",
    "  \"État du dossier\": \"etat_dossier\",\n",
    "  \"Dernière mise à jour le\": \"derniere_mise_a_jour\",\n",
    "  \"Dernière mise à jour du dossier le\": \"derniere_mise_a_jour_du_dossier\",\n",
    "  \"Déposé le\": \"depose_le\",\n",
    "  \"Passé en instruction le\": \"passe_en_instruction_le\",\n",
    "  \"Traité le\": \"traite_le\",\n",
    "  \"Motivation de la décision\": \"decision\",\n",
    "  \"Instructeurs\": \"instructeurs\",\n",
    "  \"Percevez-vous l'allocation d'éducation de l'enfant handicapé (AEEH) ?\": \"est_aeeh\",\n",
    "  \"Nom de famille de l'allocataire\": \"allocataire-nom\",\n",
    "  \"Prénom de l'allocataire\": \"allocataire-prenom\",\n",
    "  \"Adresse électronique de l'allocataire\": \"allocataire-courriel\",\n",
    "  \"L'organisme de gestion de votre allocation\": \"organisme\",\n",
    "  \"Adresse de résidence de l'allocataire\": \"adresse_allocataire-voie\",\n",
    "  \"Commune de résidence de l'allocataire\": \"adresse_allocataire-commune\",\n",
    "  \"Commune de résidence de l'allocataire (Code INSEE)\": \"adresse_allocataire-commune_insee\",\n",
    "  \"Commune de résidence de l'allocataire (Département)\": \"adresse_allocataire-departement\",\n",
    "  \"Le numéro d'allocataire CAF\": \"allocataire-matricule\",\n",
    "  \"Genre\": \"genre\",\n",
    "  \"Prénom de l'enfant\": \"prenom\",\n",
    "  \"Nom de famille de l'enfant\": \"nom\",\n",
    "  \"Date de naissance de l'enfant\": \"date_naissance\",\n",
    "  \"Attestation de paiement de l'AEEH, fournie par votre CAF ou MSA\": \"attestation_paiement\",\n",
    "  \"Nouvelle annotation\": \"annotation\"\n",
    "}\n",
    "\n",
    "df_ds = pd.read_csv(ds_input_filepath, on_bad_lines='skip', sep=',', dtype=str, engine=\"c\", keep_default_na=False, encoding=\"utf-8\")\n",
    "df_ds = df_ds.rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds['etat_dossier'] = df_ds['etat_dossier'].replace('En instruction', 'en_instruction')\n",
    "df_ds['adresse_allocataire-code-postal'] = df_ds['adresse_allocataire-commune'].str.extract(r'\\((\\d{5})\\)')\n",
    "df_ds['adresse_allocataire-commune'] = df_ds['adresse_allocataire-commune'].str.extract(r'(.+)\\s\\(\\d{5}\\)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_caf = df_ds['organisme'] == 'CAF'\n",
    "df_ds.loc[mask_caf, 'allocataire-matricule'] = df_ds.loc[mask_caf, 'allocataire-matricule'].str[:7]\n",
    "mask_matricule = df_ds['allocataire-matricule'].str.len() < 7\n",
    "df_ds.loc[mask_caf & mask_matricule, 'allocataire-matricule'] = df_ds.loc[mask_caf & mask_matricule, 'allocataire-matricule'].str.zfill(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds['allocataire-qualite'] = np.NaN\n",
    "df_ds['situation'] = 'jeune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format date_naissance to datetime python object for processing\n",
    "df_ds['date_naissance'] = pd.to_datetime(df_ds['date_naissance'], format='%Y-%m-%d')\n",
    "df_ds['prenom'] = df_ds['prenom'].apply(unaccent_and_upper).str.strip()\n",
    "df_ds['nom'] = df_ds['nom'].apply(unaccent_and_upper).str.strip()\n",
    "df_ds['genre'] = df_ds['genre'].replace({\n",
    "    'M.': 'M',\n",
    "    'Mme': 'F'\n",
    "})\n",
    "\n",
    "# add 4h on all birthdates\n",
    "df_ds['date_naissance'] = df_ds['date_naissance'] + timedelta(hours=4)\n",
    "df_ds['date_naissance'] = df_ds['date_naissance'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching against previous injected data\n",
    "df_previous_waves = pd.concat([\n",
    "    pd.read_csv('./previous_waves/2025-09-11-dossiers-a-accepter.csv', on_bad_lines='skip', sep=';', dtype=str, engine=\"c\", keep_default_na=False, encoding=\"utf-8\"),\n",
    "    pd.read_csv('./previous_waves/2025-09-11-dossiers-a-refuser-doublons.csv', on_bad_lines='skip', sep=';', dtype=str, engine=\"c\", keep_default_na=False, encoding=\"utf-8\"),\n",
    "    pd.read_csv('./previous_waves/2025-09-11-dossiers-a-refuser-non-eligibles.csv', on_bad_lines='skip', sep=';', dtype=str, engine=\"c\", keep_default_na=False, encoding=\"utf-8\")\n",
    "])\n",
    "\n",
    "df_merge_all_waves = pd.merge(df_ds, df_previous_waves, on=['prenom', 'nom' ,'date_naissance'], how='left', suffixes=('', '_prev'), indicator=True)\n",
    "\n",
    "print(f\"{len(df_ds)} folders from current wave\")\n",
    "print(f\"{len(df_previous_waves)} folders from previous waves\")\n",
    "print(f\"{len(df_merge_all_waves)} folders from merging all waves on [prenom, nom, date_naissance]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_all_waves['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df_merge_all_waves[df_merge_all_waves['_merge'] == 'both'])} total after merge in both\")\n",
    "print(f\"{len(df_merge_all_waves[df_merge_all_waves['_merge'] == 'both']) - len(df_previous_waves)} duplicated folders found in current wave against previous waves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df_merge_all_waves[df_merge_all_waves['_merge'] == 'left_only'] )} from left merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have no folders from the current wave found in previous waves\n",
    "df_merge_keep_only_current_wave = df_merge_all_waves[df_merge_all_waves['_merge'] == 'left_only']\n",
    "assert(len(df_merge_keep_only_current_wave[df_merge_keep_only_current_wave['dossier_id'].isin(df_previous_waves['dossier_id'])]) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_without_duplicated = df_merge_keep_only_current_wave.drop_duplicates(subset=['prenom', 'nom', 'date_naissance'])\n",
    "df_duplicated_folders = df_merge_keep_only_current_wave[~df_merge_keep_only_current_wave['dossier_id'].isin(df_ds_without_duplicated['dossier_id'])]\n",
    "print(f\"{len(df_duplicated_folders)} duplicated folders found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 to 13 years old\n",
    "mask_6_13_dob_start = pd.to_datetime(df_ds_without_duplicated['date_naissance']).dt.date >= datetime(2012, 1, 1).date()\n",
    "mask_6_13_dob_end = pd.to_datetime(df_ds_without_duplicated['date_naissance']).dt.date <= datetime(2019, 12, 31).date()\n",
    "\n",
    "# 18 to 20 years old\n",
    "mask_18_20_dob_start = pd.to_datetime(df_ds_without_duplicated['date_naissance']).dt.date >= datetime(2005, 1, 1).date()\n",
    "mask_18_20_dob_end = pd.to_datetime(df_ds_without_duplicated['date_naissance']).dt.date <= datetime(2007, 12, 31).date()\n",
    "\n",
    "mask_within_dates = (mask_6_13_dob_start & mask_6_13_dob_end) | (mask_18_20_dob_start & mask_18_20_dob_end)\n",
    "\n",
    "print(f\"{len(df_ds_without_duplicated)} total rows\")\n",
    "\n",
    "df_ds_eligible = df_ds_without_duplicated[mask_within_dates]\n",
    "\n",
    "print(f\"{len(df_ds_eligible)} total rows after applying dates requirements\")\n",
    "\n",
    "# Dossiers not meeting date requirements\n",
    "df_ds_not_eligible = df_ds_without_duplicated[~df_ds_without_duplicated['dossier_id'].isin(df_ds_eligible['dossier_id'])]\n",
    "print(f\"{len(df_ds_not_eligible)} total rows that do not meet dates requirements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map allocataire json\n",
    "def to_json_allocataire_without_null(row):\n",
    "    allocataire_mapping = {\n",
    "        'qualite': np.NaN,\n",
    "        'nom': unaccent_and_upper(row['allocataire-nom']).strip(),\n",
    "        'prenom': unaccent_and_upper(row['allocataire-prenom']).strip(),\n",
    "        'courriel': row['allocataire-courriel'].lower().strip()\n",
    "    }\n",
    "    if row['allocataire-matricule']:\n",
    "        allocataire_mapping['matricule'] = row['allocataire-matricule']\n",
    "    filtered_NaN_allocataire = {k: v for k, v in allocataire_mapping.items() if pd.notnull(v)}\n",
    "    return json.dumps(filtered_NaN_allocataire, ensure_ascii=False)\n",
    "\n",
    "df_ds_eligible.loc[:, 'allocataire'] = df_ds_eligible.apply(to_json_allocataire_without_null, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map adresse_allocataire json\n",
    "def to_json_adresse_without_null(row):\n",
    "    adresse_mapping = {\n",
    "        'voie': unaccent_and_upper(row['adresse_allocataire-voie'].strip()).replace('\"', '\\''),\n",
    "        'commune': unaccent_and_upper(row['adresse_allocataire-commune'].strip()),\n",
    "        'code_postal': format_insee_or_postal_code(row['adresse_allocataire-code-postal']),\n",
    "        'code_insee': format_insee_or_postal_code(row['adresse_allocataire-commune_insee'])\n",
    "    }\n",
    "    filtered_address = {k: v for k, v in adresse_mapping.items() if pd.notnull(v)}\n",
    "    return json.dumps(filtered_address, ensure_ascii=False)\n",
    "\n",
    "df_ds_eligible.loc[:, 'adresse_allocataire'] = df_ds_eligible.apply(to_json_adresse_without_null, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing default column needed to backup data\n",
    "# Put a date value for the 2025 data otherwise the merge will not work as intended (the dates from 2024 will replace the non existing dates on data from 2025)\n",
    "timestamp_with_custom_tz = pd.Timestamp.now(tz='Europe/Paris')\n",
    "timestamp_to_identify = timestamp_with_custom_tz.replace(\n",
    "    hour=23,\n",
    "    minute=23,\n",
    "    second=23,\n",
    "    microsecond=23000\n",
    ")\n",
    "\n",
    "exercice_2025 = 4\n",
    "df_ds_eligible.loc[:,['exercice_id']] = exercice_2025\n",
    "df_ds_eligible.loc[:,['id_psp', 'uuid_doc']] = np.NaN\n",
    "df_ds_eligible.loc[:,['zrr', 'qpv', 'a_valider', 'refuser']] = False\n",
    "df_ds_eligible.loc[:,['created_at', 'updated_at']] = timestamp_to_identify\n",
    "df_ds_eligible.loc[:, 'date_naissance'] = df_ds_eligible['date_naissance'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing_codes = pd.read_csv(existing_codes_filepath, on_bad_lines='skip', sep=',', dtype=str, engine=\"c\", keep_default_na=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing_codes.drop_duplicates(subset=['code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique codes generation\n",
    "import random\n",
    "import string\n",
    "import datetime\n",
    "\n",
    "df_ds_eligible = df_ds_eligible.reset_index(drop=True)\n",
    "current_date = datetime.datetime.now()\n",
    "current_year = str(current_date.year)[-2:]\n",
    "\n",
    "def get_characters_set(size = 4):\n",
    "    return ''.join(random.choices([c for c in string.ascii_uppercase if c not in 'OI'], k=size))\n",
    "\n",
    "def generate_code():\n",
    "    return f\"{current_year}-{get_characters_set(4)}-{get_characters_set(4)}\"\n",
    "\n",
    "# init set of codes with existing\n",
    "unique_codes = set(df_existing_codes['code'])\n",
    "\n",
    "# init current_code count\n",
    "current_codes_count = len(unique_codes)\n",
    "\n",
    "while len(unique_codes) < (len(df_ds_eligible) + len(df_existing_codes)):\n",
    "    unique_codes.add(generate_code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have generated codes for all the rows\n",
    "assert len(unique_codes) == (len(df_ds_eligible)+len(df_existing_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_codes = unique_codes.difference(set(df_existing_codes['code']))\n",
    "assert len(new_codes) == len(df_ds_eligible)\n",
    "len(new_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign generated code\n",
    "df_ds_eligible['id_psp'] = list(new_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(pd.merge(df_duplicated_folders, df_ds_not_eligible, how=\"inner\", on=[\"dossier_id\"])) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to CSV files folders OK\n",
    "df_ds_eligible[db_columns_with_dossier].to_csv(get_current_date_for_file_name('production-with-dossier-ids.csv'), sep=';', index=False, encoding='utf-8')\n",
    "df_ds_eligible[db_columns].to_csv(get_current_date_for_file_name('production.csv'), sep=';', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to CSV files folders NOT OK\n",
    "df_duplicated_folders[dossiers_columns].to_csv(get_current_date_for_file_name('dossiers-a-refuser-doublons.csv'), sep=';', index=False, encoding='utf-8')\n",
    "df_ds_not_eligible[dossiers_columns].to_csv(get_current_date_for_file_name('dossiers-a-refuser-non-eligibles.csv'), sep=';', index=False, encoding='utf-8')\n",
    "df_ds_eligible[dossiers_columns].to_csv(get_current_date_for_file_name('dossiers-a-accepter.csv'), sep=';', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df_ds_not_eligible)} total rows that do not meet dates requirements\")\n",
    "print(f\"{len(df_ds_eligible)} total rows after applying dates requirements\")\n",
    "print(f\"{len(df_duplicated_folders)} duplicated folders found\")\n",
    "print(f\"{len(df_ds_eligible)} folders to accept\")\n",
    "print(f\"{len(df_ds)} initial total rows\")\n",
    "print(f\"{len(df_ds_eligible) + len(df_ds_not_eligible) + len(df_duplicated_folders)} total rows processed from current wave\")\n",
    "print(f\"{len(df_ds_eligible) + len(df_ds_not_eligible) + len(df_duplicated_folders) + len(df_previous_waves)} total rows processed from all waves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds['instructeurs'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
